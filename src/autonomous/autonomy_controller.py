"""
Autonomy Controller - Cerebro del Sistema AutÃ³nomo
Control ABSOLUTO sobre todos los parÃ¡metros y decisiones
"""
import logging
import asyncio
from typing import Dict, Optional, Any, List
from datetime import datetime, timedelta
from pathlib import Path

from .rl_agent import RLAgent
from .parameter_optimizer import ParameterOptimizer
from .learning_persistence import LearningPersistence
from .git_backup import GitBackup
from .decision_brain import DecisionBrain

logger = logging.getLogger(__name__)


class AutonomyController:
    """
    Controlador AutÃ³nomo con PODER ABSOLUTO

    - Controla TODOS los parÃ¡metros sin limitaciones
    - Decide cuÃ¡ndo y quÃ© cambiar
    - Aprende de cada trade
    - Optimiza configuraciÃ³n continuamente
    - Persiste inteligencia aprendida
    - Notifica cada cambio a Telegram

    La IA tiene control TOTAL - sin restricciones humanas
    """

    def __init__(
        self,
        telegram_notifier=None,
        auto_save_interval_minutes: int = 30,
        optimization_check_interval_hours: float = 2.0,
        min_trades_before_optimization: int = 20
    ):
        """
        Args:
            telegram_notifier: Instancia de TelegramNotifier para notificaciones
            auto_save_interval_minutes: Intervalo para auto-guardar inteligencia
            optimization_check_interval_hours: Cada cuÃ¡ntas horas considerar optimizar parÃ¡metros
            min_trades_before_optimization: MÃ­nimo de trades antes de optimizar
        """
        self.telegram_notifier = telegram_notifier
        self.auto_save_interval = auto_save_interval_minutes
        self.optimization_interval = optimization_check_interval_hours
        self.min_trades_before_opt = min_trades_before_optimization

        # Componentes principales
        self.rl_agent = RLAgent(
            learning_rate=0.1,
            discount_factor=0.95,
            exploration_rate=0.3,
            exploration_decay=0.995,
            min_exploration=0.05
        )

        self.parameter_optimizer = ParameterOptimizer()

        self.persistence = LearningPersistence(storage_dir="data/autonomous")

        # Estado del sistema
        self.active = False
        self.current_parameters: Dict[str, Any] = {}
        self.performance_history: List[Dict] = []
        self.last_optimization_time = datetime.now()
        self.last_save_time = datetime.now()
        self.total_trades_processed = 0
        self.total_parameter_changes = 0

        # HistÃ³rico de cambios con razonamiento (para memoria histÃ³rica)
        self.change_history: List[Dict] = []

        # Control de decisiones
        self.decision_mode = "AUTONOMOUS"  # AUTONOMOUS, CONSERVATIVE, AGGRESSIVE

        # Git Backup System
        self.git_backup = GitBackup(
            telegram_notifier=telegram_notifier,
            backup_interval_hours=24.0,
            backup_dir="data/autonomous"
        )

        # Contador global de trades (nunca se resetea)
        self.total_trades_all_time = 0
        self.max_leverage_unlocked = 1  # Inicializar leverage

        # Referencia a market_monitor (se asigna desde main.py)
        # Necesaria para acceder a ml_system para export/import de training_buffer
        self.market_monitor = None

        # DeduplicaciÃ³n de trades (para evitar que test_mode y position_monitor notifiquen el mismo trade)
        # Dict: symbol -> (timestamp, pnl) de los Ãºltimos trades procesados
        self._recently_processed_trades: Dict[str, tuple] = {}

        # Flag para indicar si Test Mode estÃ¡ activo (para ignorar Position Monitor cuando test activo)
        self.test_mode_active = False

        # ðŸ¤– AUTONOMÃA v2.0: Aprendizaje continuo
        self.losing_streak = 0  # Contador de pÃ©rdidas consecutivas
        self.winning_streak = 0  # Contador de ganancias consecutivas
        self.recent_trades_pnl: List[float] = []  # Ãšltimos 20 trades para calcular win rate reciente
        self.temporary_adjustment = None  # Ajustes temporales cuando racha negativa

        # ðŸ§  CEREBRO CENTRAL: Decision Brain (se inicializa despuÃ©s con set_components)
        self.decision_brain = None
        self.ml_system = None
        self.trade_manager = None
        self.feature_aggregator = None
        self.sentiment_analyzer = None
        self.regime_detector = None
        self.orderbook_analyzer = None

        logger.info("ðŸ¤– AUTONOMY CONTROLLER INICIALIZADO - MODO: CONTROL ABSOLUTO")
        logger.info(f"   Auto-save: cada {self.auto_save_interval} min")
        logger.info(f"   Optimization check: cada {self.optimization_interval} horas")
        logger.info(f"   Min trades antes de optimizar: {self.min_trades_before_opt}")

    async def initialize(self):
        """
        Inicializa el controlador autÃ³nomo
        - Intenta cargar inteligencia previa
        - EnvÃ­a notificaciÃ³n de inicio
        """
        logger.info("ðŸš€ Inicializando Sistema AutÃ³nomo...")

        # Intentar cargar inteligencia guardada
        loaded_state = self.persistence.load_full_state()

        if loaded_state:
            await self._restore_from_state(loaded_state)
            await self._notify_telegram(
                "ðŸ§  **Sistema AutÃ³nomo Iniciado**\n\n"
                "âœ… Inteligencia previa CARGADA exitosamente\n"
                f"ðŸ“Š Experiencia: {self.rl_agent.total_trades} trades aprendidos\n"
                f"ðŸŽ¯ OptimizaciÃ³n: {self.parameter_optimizer.total_trials} trials completados\n"
                f"ðŸ† Mejor configuraciÃ³n restaurada\n\n"
                "El bot continuarÃ¡ aprendiendo desde donde se quedÃ³ âœ¨"
            )
        else:
            await self._notify_telegram(
                "ðŸ¤– **Sistema AutÃ³nomo Iniciado**\n\n"
                "ðŸ†• Primera ejecuciÃ³n - iniciando aprendizaje desde cero\n"
                "ðŸ§  RL Agent: Activo\n"
                "ðŸŽ¯ Parameter Optimizer: Activo\n"
                "ðŸ’¾ Auto-save: Habilitado\n\n"
                "El bot aprenderÃ¡ y se optimizarÃ¡ de forma completamente autÃ³noma ðŸš€"
            )

        self.active = True

        # Iniciar Git Auto-Backup
        await self.git_backup.start_auto_backup()

        logger.info("âœ… Sistema AutÃ³nomo ACTIVO - Control total habilitado")

    def set_components(
        self,
        ml_system=None,
        trade_manager=None,
        feature_aggregator=None,
        sentiment_analyzer=None,
        regime_detector=None,
        orderbook_analyzer=None
    ):
        """
        Configura los componentes externos para el Decision Brain

        Args:
            ml_system: Sistema de ML para predicciones
            trade_manager: Gestor de trades activos
            feature_aggregator: Agregador de features (Arsenal)
            sentiment_analyzer: Analizador de sentimiento
            regime_detector: Detector de rÃ©gimen de mercado
            orderbook_analyzer: Analizador de orderbook
        """
        self.ml_system = ml_system
        self.trade_manager = trade_manager
        self.feature_aggregator = feature_aggregator
        self.sentiment_analyzer = sentiment_analyzer
        self.regime_detector = regime_detector
        self.orderbook_analyzer = orderbook_analyzer

        # Crear el Decision Brain con todos los componentes
        self.decision_brain = DecisionBrain(
            rl_agent=self.rl_agent,
            ml_system=ml_system,
            trade_manager=trade_manager,
            parameter_optimizer=self.parameter_optimizer,
            feature_aggregator=feature_aggregator,
            sentiment_analyzer=sentiment_analyzer,
            regime_detector=regime_detector,
            orderbook_analyzer=orderbook_analyzer
        )

        logger.info("ðŸ§  Decision Brain configurado con todos los componentes")

    def _calculate_max_leverage(self) -> int:
        """
        Calcula max leverage basado en total_trades_all_time.

        MODO EXPLORACIÃ“N: Empieza con leverage 3x mÃ­nimo para permitir
        que el RL Agent explore FUTURES desde el inicio.

        Returns:
            int: Leverage mÃ¡ximo desbloqueado (3-20x)
        """
        from config import config

        total = self.total_trades_all_time

        # MODO EXPLORACIÃ“N: MÃ­nimo leverage = DEFAULT_LEVERAGE (3x)
        # Esto permite FUTURES desde el inicio para aprendizaje real
        min_leverage = getattr(config, 'DEFAULT_LEVERAGE', 3)

        if total < 10:
            return min_leverage  # Empieza con 3x (MODO EXPLORACIÃ“N)
        elif total < 20:
            return max(min_leverage, 3)
        elif total < 30:
            return max(min_leverage, 4)
        elif total < 50:
            return max(min_leverage, 5)
        elif total < 100:
            return 7
        elif total < 150:
            return 10
        elif total < 200:
            return 15
        else:
            return 20

    async def _restore_from_state(self, state: Dict, force: bool = False):
        """
        Restaura estado completo desde archivo de inteligencia
        CRÃTICO: Restaura paper trading ANTES de crear portfolio nuevo
        """
        logger.info("ðŸ“¦ Restaurando estado completo desde export...")
        logger.info(f"   Force mode: {force}")

        try:
            # ========== PASO 0: GUARDAR PAPER TRADING PARA RESTAURAR ==========
            paper_trading_to_restore = None

            if 'paper_trading' in state and state['paper_trading']:
                paper_state = state['paper_trading']

                # Verificar estructura
                if 'counters' in paper_state and 'closed_trades' in paper_state:
                    total_trades = paper_state.get('counters', {}).get('total_trades', 0)
                    closed_trades_count = len(paper_state.get('closed_trades', []))

                    if total_trades > 0:
                        logger.info(f"ðŸ“¥ Paper trading en export detectado:")
                        logger.info(f"   â€¢ Total trades: {total_trades}")
                        logger.info(f"   â€¢ Closed trades: {closed_trades_count}")
                        logger.info(f"   â€¢ Balance: ${paper_state.get('balance', 0):,.2f}")

                        # GUARDAR para restaurar DESPUÃ‰S
                        paper_trading_to_restore = paper_state
                    else:
                        logger.warning("âš ï¸ Paper trading en export pero con 0 trades")
                else:
                    logger.warning("âš ï¸ Paper trading en export pero formato incorrecto (sin counters/closed_trades)")
            else:
                logger.info("â„¹ï¸ No hay paper trading en export - se crearÃ¡ portfolio nuevo")

            # ========== PASO 1: Restaurar RL Agent ==========
            logger.info("ðŸ“¥ Paso 1/4: Restaurando RL Agent...")

            if 'rl_agent' in state:
                self.rl_agent.load_from_dict(state['rl_agent'])
                rl_stats = self.rl_agent.get_statistics()
                logger.info(f"  âœ… RL Agent restaurado")
                logger.info(f"     â€¢ {rl_stats.get('total_trades', 0)} trades")
                logger.info(f"     â€¢ {rl_stats.get('success_rate', 0):.1f}% win rate")
                logger.info(f"     â€¢ {rl_stats.get('q_table_size', 0)} estados aprendidos")

            # ========== PASO 2: Restaurar Parameter Optimizer ==========
            logger.info("ðŸ“¥ Paso 2/4: Restaurando Parameter Optimizer...")

            if 'parameter_optimizer' in state:
                self.parameter_optimizer.load_from_dict(state['parameter_optimizer'])
                logger.info(f"  âœ… Parameter Optimizer restaurado")

            # ========== PASO 3: Restaurar Change History ==========
            logger.info("ðŸ“¥ Paso 3/4: Restaurando Change History...")

            try:
                if 'change_history' in state:
                    self.change_history = state['change_history']
                    logger.info(f"  âœ… HistÃ³rico de cambios restaurado: {len(self.change_history)} cambios")
            except Exception as e:
                logger.warning(f"âš ï¸ Error restaurando change history: {e}")

            # ========== PASO 4: RESTAURAR PAPER TRADING (CRÃTICO) ==========
            logger.info("ðŸ“¥ Paso 4/4: Restaurando Paper Trading...")

            if paper_trading_to_restore:
                logger.info("ðŸ”„ Intentando restaurar paper trading desde export...")

                # VERIFICAR que paper_trader existe (v2.0: opcional)
                if not hasattr(self, 'paper_trader'):
                    logger.warning("âš ï¸ v2.0: self.paper_trader NO EXISTE (modo Binance)")
                    logger.info("   Continuando sin restaurar paper trading (normal en v2.0)...")
                elif not self.paper_trader:
                    logger.warning("âš ï¸ v2.0: self.paper_trader es None (modo Binance)")
                elif not hasattr(self.paper_trader, 'portfolio'):
                    logger.warning("âš ï¸ paper_trader.portfolio NO EXISTE")
                else:
                    # TODO LISTO - Restaurar
                    logger.info("âœ… paper_trader existe - ejecutando restore_from_state()...")

                    try:
                        success = self.paper_trader.portfolio.restore_from_state(paper_trading_to_restore)

                        if success:
                            # Verificar que realmente se restaurÃ³
                            actual_trades = self.paper_trader.portfolio.total_trades
                            actual_closed = len(self.paper_trader.portfolio.closed_trades)
                            actual_balance = self.paper_trader.portfolio.balance

                            logger.info(f"âœ… Paper Trading restaurado exitosamente:")
                            logger.info(f"   â€¢ Total trades: {actual_trades}")
                            logger.info(f"   â€¢ Closed trades: {actual_closed}")
                            logger.info(f"   â€¢ Balance: ${actual_balance:,.2f}")

                            # VerificaciÃ³n de integridad
                            expected_trades = paper_trading_to_restore['counters']['total_trades']
                            if actual_trades != expected_trades:
                                logger.warning(f"âš ï¸ Trades: esperado={expected_trades}, actual={actual_trades}")
                        else:
                            logger.error("âŒ restore_from_state() retornÃ³ False")
                            logger.error("   Revisa logs de Portfolio para mÃ¡s detalles")

                    except Exception as e:
                        logger.error(f"âŒ EXCEPCIÃ“N al ejecutar restore_from_state(): {e}", exc_info=True)
            else:
                logger.info("â„¹ï¸ No hay paper trading para restaurar - portfolio quedarÃ¡ en estado inicial")

            # ========== Restaurar metadata y otros ==========
            try:
                if 'metadata' in state:
                    metadata = state['metadata']
                    self.total_trades_all_time = metadata.get('total_trades_all_time', 0)
                    self.max_leverage_unlocked = metadata.get('max_leverage_unlocked', 1)
                    logger.info(f"  âœ… Metadata restaurada (trades totales: {self.total_trades_all_time}, max leverage: {self.max_leverage_unlocked}x)")
            except Exception as e:
                logger.warning(f"âš ï¸ Error restaurando metadata: {e}")

            # ========== Restaurar performance history ==========
            try:
                if 'performance_history' in state:
                    perf = state['performance_history']
                    logger.info(f"  âœ… Performance history restaurada ({perf.get('total_trades', 0)} entradas)")
            except Exception as e:
                logger.warning(f"âš ï¸ Error restaurando performance history: {e}")

            # ========== Restaurar ML Training Buffer ==========
            logger.info("ðŸ§  Restaurando ML Training Buffer...")

            try:
                if 'ml_training_buffer' in state:
                    buffer = state['ml_training_buffer']
                    if hasattr(self, 'ml_integration') and self.ml_integration:
                        # CÃ³digo existente para restaurar ML buffer
                        pass
                    logger.info(f"  âœ… Training buffer restaurado: {len(buffer)} features")
            except Exception as e:
                logger.warning(f"âš ï¸ Error restaurando ML buffer: {e}")

            # ========== Restaurar Trade Management Learning ==========
            logger.info("ðŸ“Š Restaurando Trade Management Learning...")

            try:
                if 'trade_management_learning' in state and state['trade_management_learning']:
                    tm_learning = state['trade_management_learning']

                    # Si trade_manager existe, restaurar directamente
                    if hasattr(self, 'trade_manager') and self.trade_manager:
                        if hasattr(self.trade_manager, 'learning'):
                            self.trade_manager.learning.stats = tm_learning.get('statistics', {})
                            self.trade_manager.learning.actions_history = tm_learning.get('actions_history', [])
                            logger.info(f"  âœ… Trade Management Learning restaurado:")
                            logger.info(f"     â€¢ {len(tm_learning.get('actions_history', []))} acciones en historial")
                            logger.info(f"     â€¢ Total evaluadas: {tm_learning.get('statistics', {}).get('total_evaluated', 0)}")
                    else:
                        # Guardar en archivo para que Trade Manager lo cargue despuÃ©s
                        from pathlib import Path
                        import json
                        filepath = 'data/trade_management_learning.json'
                        Path(filepath).parent.mkdir(parents=True, exist_ok=True)
                        with open(filepath, 'w') as f:
                            json.dump(tm_learning, f, indent=2)
                        logger.info(f"  âœ… Trade Management Learning guardado en {filepath}")
                        logger.info(f"     (Se cargarÃ¡ cuando Trade Manager inicie)")
                else:
                    logger.info("  â„¹ï¸ No hay Trade Management Learning en export")
            except Exception as e:
                logger.warning(f"âš ï¸ Error restaurando Trade Management Learning: {e}")

            # ========== VALIDACIÃ“N FINAL ==========
            logger.info("ðŸŽ¯ ValidaciÃ³n final de sincronizaciÃ³n...")

            rl_trades = self.rl_agent.get_statistics().get('total_trades', 0)
            paper_trades = self.paper_trader.portfolio.total_trades if hasattr(self, 'paper_trader') else 0

            if rl_trades != paper_trades:
                logger.warning(f"âš ï¸ DESINCRONIZACIÃ“N POST-IMPORT:")
                logger.warning(f"   RL Agent: {rl_trades} trades")
                logger.warning(f"   Paper Trading: {paper_trades} trades")
                logger.warning(f"   Diferencia: {abs(rl_trades - paper_trades)} trades")
            else:
                logger.info(f"âœ… SincronizaciÃ³n OK: {rl_trades} trades en ambos sistemas")

            logger.info("ðŸŽ‰ Inteligencia importada y restaurada completamente")

            if force:
                logger.warning("âš ï¸ ImportaciÃ³n en FORCE MODE - checksum no validado")

        except Exception as e:
            logger.error(f"âŒ Error crÃ­tico restaurando estado: {e}", exc_info=True)
            raise

    async def evaluate_trade_opportunity(
        self,
        pair: str,
        signal: Dict,
        market_state: Dict,
        portfolio_metrics: Dict
    ) -> Dict:
        """
        EvalÃºa si abrir un trade usando RL Agent ANTES de ejecutarlo

        ðŸ§  CEREBRO CENTRAL: Si DecisionBrain estÃ¡ disponible, usa anÃ¡lisis completo
        con TODOS los servicios. Si no, usa el flujo tradicional.

        Args:
            pair: Par de trading
            signal: SeÃ±al generada (BUY/SELL/HOLD)
            market_state: Estado del mercado actual
            portfolio_metrics: MÃ©tricas del portfolio

        Returns:
            Dict con decisiÃ³n del RL Agent
        """
        if not self.active:
            # Si autonomy no estÃ¡ activo, permitir el trade
            return {
                'should_trade': True,
                'action': 'OPEN',
                'position_size_multiplier': 1.0,
                'confidence': 1.0,
                'chosen_action': 'AUTONOMOUS_DISABLED'
            }

        # ðŸ” DIAGNÃ“STICO: Verificar estado de DecisionBrain
        logger.info(f"ðŸ” evaluate_trade_opportunity para {pair}")
        logger.info(f"   ðŸ“Š decision_brain disponible: {self.decision_brain is not None}")

        try:
            # ðŸ§  USAR DECISION BRAIN SI ESTÃ DISPONIBLE
            if self.decision_brain:
                logger.info(f"ðŸ§  Usando DecisionBrain para analizar {pair}")
                current_price = market_state.get('current_price', 0)
                timeframe = market_state.get('timeframe', '15m')

                # Combinar market_state con signal para anÃ¡lisis completo
                combined_data = {**market_state, **signal}
                combined_data['side'] = signal.get('action', 'NEUTRAL')

                # AnÃ¡lisis completo con todos los servicios
                analysis = self.decision_brain.analyze_opportunity(
                    symbol=pair.replace('/', ''),
                    current_price=current_price,
                    market_data=combined_data,
                    timeframe=timeframe
                )

                # Extraer decisiÃ³n final
                final_decision = analysis.get('final_decision', {})

                # Aplicar ajustes temporales si existen
                if self.temporary_adjustment and final_decision.get('action') != 'SKIP':
                    final_decision['leverage'] = int(
                        final_decision.get('leverage', 3) * self.temporary_adjustment.get('leverage_multiplier', 1.0)
                    )
                    final_decision['position_size_pct'] = (
                        final_decision.get('position_size_pct', 5.0) * self.temporary_adjustment.get('position_multiplier', 1.0)
                    )
                    if 'tp_percentages' in final_decision:
                        final_decision['tp_percentages'] = [
                            tp * self.temporary_adjustment.get('tp_multiplier', 1.0)
                            for tp in final_decision['tp_percentages']
                        ]

                # Convertir a formato esperado
                decision = {
                    'should_trade': final_decision.get('action') == 'OPEN',
                    'action': final_decision.get('action', 'SKIP'),
                    'trade_type': 'FUTURES',
                    'position_size_multiplier': final_decision.get('position_size_pct', 5.0) / 5.0,  # Normalizar a ~1.0
                    'leverage': final_decision.get('leverage', 3),
                    'confidence': final_decision.get('consolidated_confidence', 0.5),
                    'chosen_action': f"BRAIN_{final_decision.get('action', 'SKIP')}",
                    'composite_score': analysis.get('rl_decision', {}).get('composite_score', 0),
                    'tp_percentages': final_decision.get('tp_percentages', [0.5, 1.0, 1.5]),
                    'position_size_pct': final_decision.get('position_size_pct', 5.0),
                    'ml_confidence': final_decision.get('ml_confidence', 0.5),
                    'rl_confidence': final_decision.get('rl_confidence', 0.5)
                }

                logger.info(
                    f"ðŸ§  BRAIN Decision para {pair}: "
                    f"{'âœ…' if decision['should_trade'] else 'âŒ'} {decision['action']} | "
                    f"Lev={decision['leverage']}x | Conf={decision['confidence']:.1%}"
                )

                return decision

            # FALLBACK: Flujo tradicional si no hay Decision Brain
            logger.warning(f"âš ï¸ DecisionBrain NO disponible para {pair}, usando flujo tradicional")

            # Determinar side de la seÃ±al (BUY/SELL)
            signal_action = signal.get('action', 'HOLD')
            side = 'BUY' if signal_action == 'BUY' else ('SELL' if signal_action == 'SELL' else 'NEUTRAL')

            # Extraer regime strength del regime_data si estÃ¡ disponible
            regime = market_state.get('regime', 'SIDEWAYS')
            regime_strength = market_state.get('regime_strength', 'MEDIUM')

            # Si no hay regime_strength pero hay volatilidad, derivarlo
            if regime_strength == 'MEDIUM':
                volatility = market_state.get('volatility', 'medium')
                if volatility == 'high':
                    regime_strength = 'HIGH'
                elif volatility == 'low':
                    regime_strength = 'LOW'

            # Construir datos de mercado para RL Agent - INTEGRACIÃ“N COMPLETA DE LOS 16 SERVICIOS
            market_data = {
                # BÃ¡sicos
                'pair': pair,
                'side': side,
                'rsi': market_state.get('rsi', 50),
                'regime': regime,
                'regime_strength': regime_strength,
                'orderbook': market_state.get('orderbook', 'NEUTRAL'),
                'confidence': signal.get('confidence', 50),
                'total_trades': self.total_trades_all_time,  # Para tier de experiencia

                # 3. CryptoPanic GROWTH API
                'cryptopanic_sentiment': market_state.get('cryptopanic_sentiment', 'neutral'),
                'news_volume': market_state.get('news_volume', 0),
                'news_importance': market_state.get('news_importance', 0),
                'pre_pump_score': market_state.get('pre_pump_score', 0),

                # 4. Fear & Greed Index
                'fear_greed_index': market_state.get('fear_greed_index', 50),
                'fear_greed_label': market_state.get('fear_greed_label', 'neutral'),

                # 5. Sentiment Analysis
                'overall_sentiment': market_state.get('overall_sentiment', 'neutral'),
                'sentiment_strength': market_state.get('sentiment_strength', 0),
                'social_buzz': market_state.get('social_buzz', 0),

                # 6. News-Triggered Trading
                'news_triggered': market_state.get('news_triggered', False),
                'news_trigger_confidence': market_state.get('news_trigger_confidence', 0),

                # 7. Multi-Layer Confidence System
                'confidence_5m': signal.get('confidence_5m', 0),
                'confidence_1h': signal.get('confidence_1h', 0),
                'confidence_4h': signal.get('confidence_4h', 0),
                'confidence_1d': signal.get('confidence_1d', 0),
                'multi_layer_alignment': signal.get('multi_layer_alignment', 0),

                # 8. ML System (Predictor)
                'ml_prediction': signal.get('ml_prediction', 'HOLD'),
                'ml_confidence': signal.get('ml_confidence', 0),
                'ml_features_importance': signal.get('ml_features_importance', {}),

                # 12. Order Book Analyzer
                'orderbook_imbalance': market_state.get('orderbook_imbalance', 0),
                'bid_ask_spread': market_state.get('bid_ask_spread', 0),
                'orderbook_depth_score': market_state.get('orderbook_depth_score', 0),
                'market_pressure': market_state.get('market_pressure', 'NEUTRAL'),

                # 13. Market Regime Detector
                'regime_confidence': market_state.get('regime_confidence', 0),
                'trend_strength': market_state.get('trend_strength', 0),
                'volatility_regime': market_state.get('volatility_regime', 'NORMAL'),

                # 14. Dynamic TP Manager
                'dynamic_tp_multiplier': signal.get('dynamic_tp_multiplier', 1.0),
                'volatility_adjusted': signal.get('volatility_adjusted', False)
            }

            # Calcular max leverage permitido
            max_leverage = self._calculate_max_leverage()

            # RL Agent decide si abrir trade (pasando max_leverage)
            decision = self.rl_agent.decide_trade_action(market_data, max_leverage=max_leverage)

            # ðŸ¤– AUTONOMÃA v2.0: Obtener decisiÃ³n autÃ³noma con TPs dinÃ¡micos
            if decision['should_trade']:
                autonomous_decision = self.rl_agent.get_autonomous_decision(market_data)

                # Merge decisiones: usar TPs y parÃ¡metros de decisiÃ³n autÃ³noma
                if autonomous_decision['action'] != 'SKIP':
                    decision['tp_percentages'] = autonomous_decision.get('tp_percentages', [0.5, 1.0, 1.5])
                    decision['position_size_pct'] = autonomous_decision.get('position_size_pct', 5.0)
                    # Sobrescribir leverage con el dinÃ¡mico
                    decision['leverage'] = autonomous_decision.get('leverage', decision.get('leverage', 3))
                    decision['autonomous_confidence'] = autonomous_decision.get('confidence', 0.5)

                    logger.info(
                        f"ðŸ¤– RL AUTÃ“NOMO para {pair}: "
                        f"{decision['chosen_action']} | Lev={decision['leverage']}x | "
                        f"TPs=[{', '.join([f'{tp:.2f}%' for tp in decision['tp_percentages']])}]"
                    )

            logger.info(
                f"ðŸ¤– RL Evaluation para {pair}: "
                f"{decision['chosen_action']} | "
                f"Trade: {'âœ…' if decision['should_trade'] else 'âŒ'} | "
                f"Size: {decision['position_size_multiplier']:.1f}x"
            )

            return decision

        except Exception as e:
            logger.error(f"âŒ Error en evaluate_trade_opportunity: {e}", exc_info=True)
            # En caso de error, permitir el trade
            return {
                'should_trade': True,
                'action': 'OPEN',
                'position_size_multiplier': 1.0,
                'confidence': 1.0,
                'chosen_action': 'ERROR_FALLBACK'
            }

    async def process_trade_outcome(
        self,
        trade_data: Dict,
        market_state: Dict,
        portfolio_metrics: Dict
    ):
        """
        Procesa resultado de un trade y aprende de Ã©l

        Args:
            trade_data: Datos del trade (pair, action, profit_pct, etc.)
            market_state: Estado del mercado (indicadores, regime, sentiment, etc.)
            portfolio_metrics: MÃ©tricas del portfolio (win_rate, roi, etc.)
        """
        if not self.active:
            return

        # Incrementar contador global de trades (nunca se resetea)
        self.total_trades_all_time += 1

        # Calcular reward basado en resultado del trade
        reward = self._calculate_reward(trade_data, portfolio_metrics)

        # LOG CRÃTICO para debugging: Verificar que reward se calcula correctamente
        logger.info(
            f"ðŸŽ“ RL LEARNING: {trade_data.get('pair')} | "
            f"P&L: {trade_data.get('profit_pct', 0):+.2f}% | "
            f"Leverage: {trade_data.get('leverage', 1)}x | "
            f"Reward calculado: {reward:+.3f}"
        )

        # Convertir estado de mercado a representaciÃ³n para RL
        state = self.rl_agent.get_state_representation(market_state)

        # Determinar si el episodio termina (grandes wins/losses)
        profit_pct = trade_data.get('profit_pct', 0)
        done = (profit_pct > 20) or (profit_pct < -10)  # Episodio termina en extremos

        # RL Agent aprende del trade
        self.rl_agent.learn_from_trade(reward=reward, next_state=state, done=done)

        # Experience Replay periÃ³dico
        if self.rl_agent.total_trades % 10 == 0:
            self.rl_agent.replay_experience(batch_size=32)

        # Guardar en historial
        self.performance_history.append({
            'timestamp': datetime.now().isoformat(),
            'trade': trade_data,
            'market_state': market_state,
            'portfolio_metrics': portfolio_metrics,
            'reward': reward
        })

        self.total_trades_processed += 1

        # Log de desbloqueo de leverage
        max_leverage = self._calculate_max_leverage()
        if self.total_trades_all_time in [50, 100, 150, 500]:
            await self._notify_telegram(
                f"ðŸŽ‰ **Nuevo Leverage Desbloqueado**\n\n"
                f"Total trades: {self.total_trades_all_time}\n"
                f"Max leverage: {max_leverage}x\n"
                f"Â¡El RL Agent ahora puede usar futuros con mayor leverage!"
            )

        # =============================================
        # ðŸ¤– AUTONOMÃA v2.0: APRENDIZAJE CONTINUO DINÃMICO
        # =============================================

        # Actualizar rachas de ganancia/pÃ©rdida
        profit_pct = trade_data.get('profit_pct', 0)
        if profit_pct > 0:
            self.winning_streak += 1
            self.losing_streak = 0
            # Limpiar ajustes temporales en ganancia
            if self.temporary_adjustment:
                logger.info(f"âœ… Racha ganadora ({self.winning_streak}), limpiando ajustes temporales")
                self.temporary_adjustment = None
        else:
            self.losing_streak += 1
            self.winning_streak = 0

        # Actualizar historial reciente (Ãºltimos 20 trades)
        self.recent_trades_pnl.append(profit_pct)
        if len(self.recent_trades_pnl) > 20:
            self.recent_trades_pnl = self.recent_trades_pnl[-20:]

        # Ajustar exploraciÃ³n basado en win rate reciente
        if len(self.recent_trades_pnl) >= 10:
            recent_wins = sum(1 for pnl in self.recent_trades_pnl if pnl > 0)
            recent_win_rate = (recent_wins / len(self.recent_trades_pnl)) * 100

            if recent_win_rate > 85:
                # Alto win rate: reducir exploraciÃ³n, confiar mÃ¡s
                new_exploration = max(self.rl_agent.min_exploration, self.rl_agent.exploration_rate * 0.95)
                if new_exploration != self.rl_agent.exploration_rate:
                    logger.info(f"ðŸ“ˆ Win rate alto ({recent_win_rate:.1f}%), reduciendo exploraciÃ³n: {self.rl_agent.exploration_rate:.2f} â†’ {new_exploration:.2f}")
                    self.rl_agent.exploration_rate = new_exploration

            elif recent_win_rate < 60:
                # Win rate bajo: aumentar exploraciÃ³n, buscar mejores estrategias
                new_exploration = min(0.4, self.rl_agent.exploration_rate * 1.05)
                if new_exploration != self.rl_agent.exploration_rate:
                    logger.info(f"ðŸ“‰ Win rate bajo ({recent_win_rate:.1f}%), aumentando exploraciÃ³n: {self.rl_agent.exploration_rate:.2f} â†’ {new_exploration:.2f}")
                    self.rl_agent.exploration_rate = new_exploration

        # Ajustes temporales en racha perdedora
        if self.losing_streak >= 3:
            logger.warning(f"âš ï¸ Racha de {self.losing_streak} pÃ©rdidas consecutivas, ajustando estrategia temporalmente")
            self.temporary_adjustment = {
                'leverage_multiplier': 0.5,  # Reducir leverage 50%
                'tp_multiplier': 0.8,        # TPs mÃ¡s cercanos
                'position_multiplier': 0.7   # Posiciones mÃ¡s pequeÃ±as
            }
            await self._notify_telegram(
                f"âš ï¸ **Ajuste Temporal de Estrategia**\n\n"
                f"Racha perdedora: {self.losing_streak} trades\n"
                f"Leverage: -50%\n"
                f"Position size: -30%\n"
                f"TPs mÃ¡s cercanos\n\n"
                f"Se revertirÃ¡ automÃ¡ticamente con prÃ³xima ganancia"
            )

        # Notificar aprendizaje importante
        if reward > 2.0:  # Gran ganancia
            await self._notify_telegram(
                f"ðŸŽ‰ **Trade Exitoso Aprendido**\n\n"
                f"Par: {trade_data.get('pair', 'N/A')}\n"
                f"Profit: {trade_data.get('profit_pct', 0):.2f}%\n"
                f"Reward: {reward:.3f}\n"
                f"Win streak: {self.winning_streak} âœ¨"
            )
        elif reward < -2.0:  # Gran pÃ©rdida
            await self._notify_telegram(
                f"ðŸ“š **Trade Perdedor Analizado**\n\n"
                f"Par: {trade_data.get('pair', 'N/A')}\n"
                f"Loss: {trade_data.get('profit_pct', 0):.2f}%\n"
                f"Reward: {reward:.3f}\n"
                f"Lose streak: {self.losing_streak} âš™ï¸"
            )

        # Verificar si es momento de optimizar parÃ¡metros
        await self._check_and_optimize(portfolio_metrics)

        # Auto-save periÃ³dico
        await self._auto_save_if_needed()

    async def process_trade_closure(self, trade_info: Dict):
        """
        ðŸ§  MÃ‰TODO CENTRALIZADO: Procesa cierre de trade y actualiza TODOS los sistemas IA

        Este mÃ©todo DEBE ser llamado cada vez que un trade se cierra, ya sea por:
        - Take Profit (TP)
        - Stop Loss (SL)
        - Cierre manual
        - LiquidaciÃ³n

        Args:
            trade_info: Dict con informaciÃ³n completa del trade:
                - symbol: Par trading (ej: BTCUSDT)
                - side: LONG/SHORT
                - entry_price: Precio de entrada
                - exit_price: Precio de salida
                - pnl_pct: % de ganancia/pÃ©rdida (ROE)
                - pnl_usdt: P&L en USDT
                - leverage: Leverage usado
                - reason: RazÃ³n del cierre (TP_HIT, SL_HIT, MANUAL, etc.)
                - duration: DuraciÃ³n en segundos (opcional)
        """
        symbol = trade_info.get('symbol', 'UNKNOWN')
        pnl_pct = trade_info.get('pnl_pct', 0)
        pnl_usdt = trade_info.get('pnl_usdt', 0)
        reason = trade_info.get('reason', 'UNKNOWN')
        leverage = trade_info.get('leverage', 1)

        logger.info(f"ðŸ”„ PROCESS_TRADE_CLOSURE: {symbol} | {reason} | P&L: {pnl_pct:+.2f}% (${pnl_usdt:+.2f})")

        # ===============================================
        # 1. ACTUALIZAR RL AGENT (Q-Table)
        # ===============================================
        try:
            if self.rl_agent:
                # Calcular reward
                reward = pnl_pct * leverage

                # El RL Agent aprende del resultado
                self.rl_agent.learn_from_trade(reward=reward, next_state=None, done=True)

                logger.info(f"   ðŸ“š RL Agent actualizado: Reward={reward:+.2f}, Q-table size={len(self.rl_agent.q_table)}")

                # Experience Replay cada 5 trades
                if self.rl_agent.total_trades % 5 == 0:
                    self.rl_agent.replay_experience(batch_size=16)
                    logger.info(f"   ðŸ”„ Experience Replay ejecutado")
        except Exception as e:
            logger.error(f"   âŒ Error actualizando RL Agent: {e}")

        # ===============================================
        # 2. ACTUALIZAR ML SYSTEM
        # ===============================================
        try:
            if self.ml_system and hasattr(self.ml_system, 'add_trade_result'):
                self.ml_system.add_trade_result(trade_info)
                logger.info(f"   ðŸ§  ML System actualizado con resultado")
        except Exception as e:
            logger.error(f"   âŒ Error actualizando ML System: {e}")

        # ===============================================
        # 3. ACTUALIZAR DECISION BRAIN
        # ===============================================
        try:
            if self.decision_brain:
                self.decision_brain.learn_from_trade(trade_info)
                logger.info(f"   ðŸ§  Decision Brain aprendiÃ³ del trade")
        except Exception as e:
            logger.error(f"   âŒ Error actualizando Decision Brain: {e}")

        # ===============================================
        # 4. NOTIFICAR A PARAMETER OPTIMIZER
        # ===============================================
        try:
            if self.parameter_optimizer:
                # Registrar resultado para el optimizador
                self.parameter_optimizer.record_trial_result(
                    config={'leverage': leverage, 'symbol': symbol},
                    performance={
                        'pnl_pct': pnl_pct,
                        'win_rate': self.rl_agent.get_success_rate() if self.rl_agent else 0,
                        'total_trades': self.total_trades_all_time
                    }
                )
                logger.info(f"   âš™ï¸ Parameter Optimizer notificado")
        except Exception as e:
            logger.error(f"   âŒ Error notificando Parameter Optimizer: {e}")

        # ===============================================
        # 5. ACTUALIZAR ESTADÃSTICAS GLOBALES
        # ===============================================
        self.total_trades_all_time += 1
        self.total_trades_processed += 1

        # Actualizar rachas
        if pnl_pct > 0:
            self.winning_streak += 1
            self.losing_streak = 0
            if self.temporary_adjustment:
                logger.info(f"   âœ… Racha ganadora ({self.winning_streak}), limpiando ajustes temporales")
                self.temporary_adjustment = None
        else:
            self.losing_streak += 1
            self.winning_streak = 0

        # Actualizar historial reciente
        self.recent_trades_pnl.append(pnl_pct)
        if len(self.recent_trades_pnl) > 20:
            self.recent_trades_pnl = self.recent_trades_pnl[-20:]

        # ===============================================
        # 6. NOTIFICAR A TELEGRAM (APRENDIZAJE)
        # ===============================================
        try:
            # Calcular mÃ©tricas para el mensaje
            win_rate = self.rl_agent.get_success_rate() if self.rl_agent else 0
            q_table_size = len(self.rl_agent.q_table) if self.rl_agent else 0

            # Determinar quÃ© aprendiÃ³ el sistema
            learning_insight = ""
            if pnl_pct > 0:
                if leverage >= 5:
                    learning_insight = f"âœ¨ AprendÃ­: {symbol} con {leverage}x leverage funciona bien en estas condiciones"
                else:
                    learning_insight = f"âœ¨ AprendÃ­: {symbol} es rentable con estrategia conservadora"
            else:
                if abs(pnl_pct) > 5:
                    learning_insight = f"ðŸ“š AprendÃ­: Evitar {symbol} en condiciones similares con {leverage}x"
                else:
                    learning_insight = f"ðŸ“š AprendÃ­: Ajustar SL/TP para {symbol}"

            # Construir mensaje de notificaciÃ³n
            await self._notify_telegram(
                f"ðŸ§  **Trade Cerrado - Aprendizaje**\n\n"
                f"Par: {symbol}\n"
                f"Resultado: {pnl_pct:+.2f}% (${pnl_usdt:+.2f})\n"
                f"RazÃ³n: {reason}\n"
                f"Leverage: {leverage}x\n\n"
                f"ðŸ“Š **ActualizaciÃ³n IA:**\n"
                f"â€¢ Q-table: {q_table_size} estados\n"
                f"â€¢ Win rate: {win_rate:.1f}%\n"
                f"â€¢ Trades totales: {self.total_trades_all_time}\n"
                f"â€¢ Racha: {'ðŸ”¥' + str(self.winning_streak) + ' wins' if self.winning_streak > 0 else 'â„ï¸' + str(self.losing_streak) + ' losses'}\n\n"
                f"{learning_insight}"
            )
        except Exception as e:
            logger.error(f"   âŒ Error enviando notificaciÃ³n Telegram: {e}")

        # ===============================================
        # 7. AUTO-SAVE SI ES NECESARIO
        # ===============================================
        if self.total_trades_processed % 10 == 0:
            await self._auto_save_if_needed()

        logger.info(f"âœ… PROCESS_TRADE_CLOSURE completado para {symbol}")

    def _calculate_reward(self, trade_data: Dict, portfolio_metrics: Dict) -> float:
        """
        Calcula reward para el RL Agent

        Para SPOT:
        - Profit/loss del trade con ajustes por mÃ©tricas del portfolio

        Para FUTURES:
        - Profit/loss * leverage (simple, sin penalizaciones artificiales)
        - El RL debe aprender por sÃ­ mismo que liquidarse es malo
        """
        profit_pct = trade_data.get('profit_pct', 0)
        trade_type = trade_data.get('trade_type', 'FUTURES')  # Default FUTURES
        leverage = trade_data.get('leverage', 1)
        liquidated = trade_data.get('liquidated', False)

        # FUTUROS: reward simple = profit_pct * leverage
        if trade_type == 'FUTURES':
            # Para liquidaciones, el profit_pct ya es -100%, asÃ­ que el reward es muy negativo
            reward = profit_pct * leverage

            # NO agregamos penalizaciones artificiales - que aprenda de la experiencia real
            logger.debug(f"Futures reward: {profit_pct:.2f}% * {leverage}x = {reward:.2f}")

        else:
            # SPOT: Base reward con ajustes
            reward = profit_pct

            # Bonus/penalty por mÃ©tricas de portfolio
            win_rate = portfolio_metrics.get('win_rate', 50)
            if win_rate > 55:
                reward *= 1.2  # Bonus si win rate es bueno
            elif win_rate < 45:
                reward *= 0.8  # Penalty si win rate es malo

            # Penalty por drawdown alto
            drawdown = abs(portfolio_metrics.get('max_drawdown', 0))
            if drawdown > 15:
                reward -= 0.5  # Penalty adicional

            # Bonus por Sharpe ratio alto
            sharpe = portfolio_metrics.get('sharpe_ratio', 0)
            if sharpe > 1.5:
                reward += 0.3

        return reward

    async def _check_and_optimize(self, portfolio_metrics: Dict):
        """
        Verifica si es momento de optimizar parÃ¡metros
        DecisiÃ³n AUTÃ“NOMA - la IA decide cuÃ¡ndo optimizar
        """
        # Verificar condiciones para optimizar
        elapsed_hours = (datetime.now() - self.last_optimization_time).total_seconds() / 3600

        should_optimize = False
        reason = ""

        # CondiciÃ³n 1: Ha pasado suficiente tiempo
        if elapsed_hours >= self.optimization_interval:
            should_optimize = True
            reason = f"Intervalo de tiempo alcanzado ({elapsed_hours:.1f} horas)"

        # CondiciÃ³n 2: Suficientes trades procesados
        if self.total_trades_processed >= self.min_trades_before_opt:
            should_optimize = True
            reason = f"Suficientes trades procesados ({self.total_trades_processed})"

        # CondiciÃ³n 3: Performance muy mala (intervenciÃ³n urgente)
        if portfolio_metrics.get('win_rate', 50) < 35:
            should_optimize = True
            reason = f"âš ï¸ Win rate crÃ­tico ({portfolio_metrics['win_rate']:.1f}%) - optimizaciÃ³n urgente"

        if portfolio_metrics.get('roi', 0) < -10:
            should_optimize = True
            reason = f"âš ï¸ ROI crÃ­tico ({portfolio_metrics['roi']:.1f}%) - optimizaciÃ³n urgente"

        # CondiciÃ³n 4: Performance muy buena (aprovechar momentum)
        if portfolio_metrics.get('win_rate', 50) > 65:
            should_optimize = True
            reason = f"ðŸŽ¯ Win rate excelente ({portfolio_metrics['win_rate']:.1f}%) - optimizar para maximizar"

        if should_optimize:
            await self._optimize_parameters(portfolio_metrics, reason)

    async def _optimize_parameters(self, portfolio_metrics: Dict, reason: str):
        """
        Optimiza parÃ¡metros de forma autÃ³noma
        LA IA DECIDE QUÃ‰ CAMBIAR - CONTROL ABSOLUTO
        """
        logger.info(f"ðŸŽ¯ INICIANDO OPTIMIZACIÃ“N AUTÃ“NOMA: {reason}")

        # Calcular exploration factor dinÃ¡mico
        # MÃ¡s exploraciÃ³n si performance es mala, menos si es buena
        win_rate = portfolio_metrics.get('win_rate', 50)
        if win_rate < 40:
            exploration_factor = 0.5  # Mucha exploraciÃ³n
        elif win_rate > 60:
            exploration_factor = 0.2  # Poca exploraciÃ³n (ya va bien)
        else:
            exploration_factor = 0.3  # ExploraciÃ³n moderada

        # Obtener sugerencias de cambios
        suggestions = self.parameter_optimizer.suggest_parameter_changes(
            current_performance=portfolio_metrics,
            exploration_factor=exploration_factor
        )

        new_config = suggestions['config']
        changes = suggestions['changes']
        strategy = suggestions['strategy']
        change_reason = suggestions['reason']

        # Aplicar cambios
        self.current_parameters.update(new_config)
        self.parameter_optimizer.current_config = new_config

        # Registrar trial
        self.parameter_optimizer.record_trial_result(new_config, portfolio_metrics)

        self.last_optimization_time = datetime.now()
        self.total_parameter_changes += 1

        # GUARDAR EN HISTÃ“RICO DE CAMBIOS (memoria histÃ³rica para futuro)
        change_record = {
            'timestamp': datetime.now().isoformat(),
            'change_number': self.total_parameter_changes,
            'trigger_reason': reason,
            'strategy': strategy,
            'performance_before': {
                'win_rate': portfolio_metrics.get('win_rate', 0),
                'roi': portfolio_metrics.get('roi', 0),
                'sharpe_ratio': portfolio_metrics.get('sharpe_ratio', 0),
                'max_drawdown': portfolio_metrics.get('max_drawdown', 0)
            },
            'parameters_changed': [
                {
                    'parameter': c['parameter'],
                    'old_value': c['old_value'],
                    'new_value': c['new_value'],
                    'change_pct': c['change_pct']
                }
                for c in changes
            ],
            'reasoning': change_reason,
            'exploration_factor': exploration_factor,
            'total_trades_at_change': self.total_trades_processed
        }
        self.change_history.append(change_record)

        # Mantener solo Ãºltimos 100 cambios (para no saturar)
        if len(self.change_history) > 100:
            self.change_history = self.change_history[-100:]

        # NOTIFICAR CAMBIOS A TELEGRAM
        await self._notify_parameter_changes(changes, strategy, change_reason, portfolio_metrics)

        logger.info(f"âœ… OptimizaciÃ³n completada: {len(changes)} parÃ¡metros modificados")

    async def _notify_parameter_changes(
        self,
        changes: List[Dict],
        strategy: str,
        reason: str,
        metrics: Dict
    ):
        """
        Notifica cambios de parÃ¡metros a Telegram (versiÃ³n resumida)
        Cada modificaciÃ³n que la IA hace es notificada de forma concisa
        """
        if not self.telegram_notifier:
            return

        # VERSIÃ“N RESUMIDA (mÃ¡s concisa para no saturar)
        message_parts = [
            f"ðŸ¤– <b>IA realizÃ³ {len(changes)} cambios</b> ({strategy})",
            "",
            f"ðŸ“Š Performance: Win Rate {metrics.get('win_rate', 0):.1f}% | ROI {metrics.get('roi', 0):+.2f}%",
        ]

        # RazÃ³n resumida (max 120 caracteres)
        reason_lines = reason.split('\n')
        brief_reason = reason_lines[0] if reason_lines else reason
        if len(brief_reason) > 120:
            brief_reason = brief_reason[:117] + "..."
        message_parts.append(f"ðŸ’¡ {brief_reason}")

        # Top 3-5 cambios mÃ¡s significativos solamente
        sorted_changes = sorted(changes, key=lambda x: abs(x.get('change_pct', 0)), reverse=True)
        top_changes = sorted_changes[:5]

        if top_changes:
            message_parts.append("")
            message_parts.append("ðŸ”§ <b>Principales cambios:</b>")
            for change in top_changes:
                param = change['parameter']
                new = change['new_value']
                direction = "ðŸ“ˆ" if change.get('change_pct', 0) > 0 else "ðŸ“‰"

                # Nombre mÃ¡s corto para parÃ¡metros comunes
                param_short = param.replace('_THRESHOLD', '').replace('_PCT', '').replace('_PERIOD', '')
                message_parts.append(f"  {direction} {param_short}: {new}")

        if len(changes) > 5:
            message_parts.append(f"  ... +{len(changes) - 5} parÃ¡metros mÃ¡s")

        message_parts.extend([
            "",
            f"ðŸ§  Total: {self.total_parameter_changes} cambios | {self.total_trades_processed} trades"
        ])

        message = "\n".join(message_parts)

        try:
            await self.telegram_notifier.send_status_message(message)
        except Exception as e:
            logger.warning(f"No se pudo enviar notificaciÃ³n: {e}")

    async def _auto_save_if_needed(self):
        """Auto-guarda inteligencia si ha pasado suficiente tiempo"""
        elapsed_minutes = (datetime.now() - self.last_save_time).total_seconds() / 60

        if elapsed_minutes >= self.auto_save_interval:
            await self.save_intelligence()
            self.last_save_time = datetime.now()

    async def save_intelligence(self) -> str:
        """
        Guarda toda la inteligencia aprendida

        Returns:
            Path al archivo de exportaciÃ³n, o string vacÃ­o si fallÃ³
        """
        logger.info("ðŸ’¾ Guardando inteligencia aprendida...")

        # Validar sincronizaciÃ³n ANTES de exportar
        sync_status = self.validate_sync()
        if not sync_status['in_sync']:
            logger.warning(
                f"âš ï¸ ADVERTENCIA: Exportando con desincronizaciÃ³n\n"
                f"   Paper Trading: {sync_status['paper_trades']} trades\n"
                f"   RL Agent: {sync_status['rl_trades']} trades\n"
                f"   El export contendrÃ¡ esta desincronizaciÃ³n"
            )

        rl_state = self.rl_agent.save_to_dict()
        optimizer_state = self.parameter_optimizer.save_to_dict()

        performance_summary = {
            'total_trades': len(self.performance_history),
            'recent_performance': self.performance_history[-100:] if self.performance_history else []
        }

        metadata = {
            'current_parameters': self.current_parameters,
            'total_trades_processed': self.total_trades_processed,
            'total_trades_all_time': self.total_trades_all_time,  # Contador global nunca se resetea
            'max_leverage_unlocked': self._calculate_max_leverage(),
            'total_parameter_changes': self.total_parameter_changes,
            'last_optimization': self.last_optimization_time.isoformat(),
            'decision_mode': self.decision_mode
        }

        # Guardar estado de paper trading si existe (TODO EL HISTORIAL)
        paper_trading_state = None
        if hasattr(self, 'paper_trader') and self.paper_trader:
            paper_trading_state = self.paper_trader.portfolio.get_full_state_for_export()
            logger.debug(
                f"ðŸ“¤ Exportando paper trading: "
                f"{len(paper_trading_state.get('closed_trades', []))} trades, "
                f"{paper_trading_state['counters']['total_trades']} total histÃ³rico"
            )

        # Guardar training_buffer del ML System si existe
        ml_training_buffer = []
        if hasattr(self, 'market_monitor') and self.market_monitor:
            if hasattr(self.market_monitor, 'ml_system') and self.market_monitor.ml_system:
                ml_system = self.market_monitor.ml_system
                if hasattr(ml_system, 'training_buffer'):
                    ml_training_buffer = ml_system.training_buffer
                    logger.debug(f"ðŸ§  ML Training Buffer incluido en export: {len(ml_training_buffer)} features")

        # Guardar learning del Trade Manager
        trade_management_learning = self.get_trade_management_learning_data()
        if trade_management_learning:
            logger.info(f"ðŸ“Š Trade Management Learning incluido: {trade_management_learning.get('total_actions_recorded', 0)} acciones")

        # ðŸ§  CRÃTICO: Guardar estado del DecisionBrain
        decision_brain_state = None
        if hasattr(self, 'decision_brain') and self.decision_brain:
            decision_brain_state = self.decision_brain.get_state()
            logger.info(f"ðŸ§  DecisionBrain state incluido: {decision_brain_state.get('trades_analyzed', 0)} trades analizados")

        success = self.persistence.save_full_state(
            rl_agent_state=rl_state,
            optimizer_state=optimizer_state,
            performance_history=performance_summary,
            change_history=self.change_history,  # HistÃ³rico de cambios con razonamiento
            metadata=metadata,
            paper_trading=paper_trading_state,  # NUEVO: incluir paper trading
            ml_training_buffer=ml_training_buffer,  # NUEVO: incluir training buffer
            trade_management_learning=trade_management_learning,  # NUEVO: incluir learning del Trade Manager
            decision_brain_state=decision_brain_state  # ðŸ§  NUEVO: incluir estado del Brain
        )

        if success:
            logger.info("âœ… Inteligencia guardada exitosamente")

            # Exportar para fÃ¡cil importaciÃ³n
            export_path = self.persistence.export_for_import()
            if export_path:
                logger.info(f"ðŸ“¤ Archivo de exportaciÃ³n: {export_path}")
                return export_path
        else:
            logger.error("âŒ Error guardando inteligencia")

        return ""

    def get_current_parameters(self) -> Dict[str, Any]:
        """Retorna parÃ¡metros actuales (para aplicar en el bot)"""
        return self.current_parameters.copy()

    def validate_sync(self) -> Dict:
        """
        Valida sincronizaciÃ³n entre TODOS los contadores de trades:
        - Paper Trading
        - RL Agent
        - AutonomyController (total_trades_processed y total_trades_all_time)
        - Win Rate entre Paper Trading y RL Agent
        """
        paper_trades = 0
        rl_trades = 0
        paper_win_rate = 0.0
        rl_win_rate = 0.0
        processed_trades = self.total_trades_processed
        all_time_trades = self.total_trades_all_time

        # Obtener conteos de cada sistema
        if hasattr(self, 'paper_trader') and self.paper_trader:
            paper_trades = self.paper_trader.portfolio.total_trades
            paper_stats = self.paper_trader.portfolio.get_statistics()
            paper_win_rate = paper_stats.get('win_rate', 0)

        if hasattr(self, 'rl_agent') and self.rl_agent:
            rl_stats = self.rl_agent.get_statistics()
            rl_trades = rl_stats.get('total_trades', 0)
            rl_win_rate = rl_stats.get('success_rate', 0)

        # Verificar sincronizaciÃ³n completa (TODOS los contadores deben coincidir)
        # Win rate puede tener diferencia de hasta 1% por redondeo
        win_rate_in_sync = abs(paper_win_rate - rl_win_rate) < 1.0

        in_sync = (paper_trades == rl_trades and
                   paper_trades == processed_trades and
                   paper_trades == all_time_trades and
                   win_rate_in_sync)

        if not in_sync:
            logger.error(
                f"ðŸš¨ DESINCRONIZACIÃ“N DETECTADA:\n"
                f"   Paper Trading: {paper_trades} trades, {paper_win_rate:.1f}% win rate\n"
                f"   RL Agent: {rl_trades} trades, {rl_win_rate:.1f}% win rate\n"
                f"   Trades Procesados: {processed_trades}\n"
                f"   Total All Time: {all_time_trades}\n"
                f"   Usa /force_sync para sincronizar todos"
            )
        else:
            logger.debug(f"âœ… SincronizaciÃ³n OK: {paper_trades} trades, {paper_win_rate:.1f}% win rate en TODOS los contadores")

        return {
            'in_sync': in_sync,
            'paper_trades': paper_trades,
            'rl_trades': rl_trades,
            'processed_trades': processed_trades,
            'all_time_trades': all_time_trades,
            'paper_win_rate': paper_win_rate,
            'rl_win_rate': rl_win_rate,
            'win_rate_in_sync': win_rate_in_sync,
            'differences': {
                'rl_vs_paper': abs(rl_trades - paper_trades),
                'processed_vs_paper': abs(processed_trades - paper_trades),
                'all_time_vs_paper': abs(all_time_trades - paper_trades),
                'win_rate_diff': abs(paper_win_rate - rl_win_rate)
            }
        }

    async def force_sync_from_paper(self) -> bool:
        """
        FUERZA sincronizaciÃ³n usando Paper Trading como fuente de verdad

        ADVERTENCIA: Esto ajustarÃ¡ TODOS los contadores al Paper Trading,
        pero NO borrarÃ¡ el conocimiento aprendido (Q-table se mantiene).

        Sincroniza:
        - RL Agent total_trades y successful_trades
        - AutonomyController total_trades_processed
        - AutonomyController total_trades_all_time

        Returns:
            True si sincronizaciÃ³n fue exitosa
        """
        if not hasattr(self, 'paper_trader') or not self.paper_trader:
            logger.error("âŒ Paper trader no disponible")
            return False

        if not hasattr(self, 'rl_agent') or not self.rl_agent:
            logger.error("âŒ RL Agent no disponible")
            return False

        # Obtener conteos actuales
        paper_trades = self.paper_trader.portfolio.total_trades
        rl_trades = self.rl_agent.total_trades
        processed_trades = self.total_trades_processed
        all_time_trades = self.total_trades_all_time

        # Obtener win rates
        paper_stats = self.paper_trader.portfolio.get_statistics()
        paper_win_rate = paper_stats['win_rate']
        rl_win_rate = self.rl_agent.get_success_rate()

        # Verificar si ya estÃ¡n sincronizados TODOS los contadores Y WIN RATE
        trades_in_sync = (paper_trades == rl_trades and
                         paper_trades == processed_trades and
                         paper_trades == all_time_trades)
        win_rate_in_sync = abs(paper_win_rate - rl_win_rate) < 1.0  # Tolerancia de 1%

        if trades_in_sync and win_rate_in_sync:
            logger.info(f"âœ… Ya estÃ¡n sincronizados todos los contadores y win rate ({paper_win_rate:.1f}%), no se requiere acciÃ³n")
            return True

        # Si solo los contadores estÃ¡n sincronizados pero no el win rate
        if trades_in_sync and not win_rate_in_sync:
            logger.warning(
                f"âš ï¸ Contadores sincronizados pero WIN RATE desincronizado:\n"
                f"   Paper Trading: {paper_trades} trades, {paper_win_rate:.1f}% WR\n"
                f"   RL Agent: {rl_trades} trades, {rl_win_rate:.1f}% WR\n"
                f"   FORZANDO SINCRONIZACIÃ“N DE WIN RATE..."
            )

        logger.warning(
            f"âš ï¸ FORZANDO SINCRONIZACIÃ“N COMPLETA:\n"
            f"   Paper Trading: {paper_trades} trades (FUENTE DE VERDAD)\n"
            f"   \n"
            f"   ANTES:\n"
            f"   â€¢ RL Agent: {rl_trades} trades\n"
            f"   â€¢ Trades Procesados: {processed_trades}\n"
            f"   â€¢ Total All Time: {all_time_trades}\n"
            f"   \n"
            f"   DESPUÃ‰S (todos ajustados a {paper_trades}):"
        )

        # 1. Ajustar contador del RL Agent
        old_rl_trades = self.rl_agent.total_trades
        old_successful = self.rl_agent.successful_trades
        self.rl_agent.total_trades = paper_trades

        # 2. Ajustar successful_trades usando SIEMPRE Paper Trading como fuente de verdad
        # paper_stats y paper_win_rate ya calculados arriba en la lÃ­nea 941-942
        new_successful = int(paper_trades * paper_win_rate / 100)

        logger.info(
            f"ðŸ”„ Actualizando RL Agent:\n"
            f"   ANTES: total_trades={old_rl_trades}, successful_trades={old_successful} ({(old_successful/old_rl_trades*100) if old_rl_trades > 0 else 0:.1f}% WR)\n"
            f"   DESPUÃ‰S: total_trades={paper_trades}, successful_trades={new_successful} ({paper_win_rate:.1f}% WR)"
        )

        self.rl_agent.successful_trades = new_successful

        # Verificar que se actualizÃ³ correctamente
        actual_wr = self.rl_agent.get_success_rate()
        logger.info(f"âœ… VerificaciÃ³n: RL Agent ahora tiene {self.rl_agent.successful_trades}/{self.rl_agent.total_trades} = {actual_wr:.1f}% WR")

        # 3. Ajustar contadores del AutonomyController
        self.total_trades_processed = paper_trades
        self.total_trades_all_time = paper_trades

        logger.info(f"âœ… SincronizaciÃ³n forzada completada - TODOS los contadores:")
        logger.info(f"   â€¢ Paper Trading: {paper_trades} trades âœ…")
        logger.info(f"   â€¢ RL Agent: {self.rl_agent.total_trades} trades âœ…")
        logger.info(f"   â€¢ Trades Procesados: {self.total_trades_processed} âœ…")
        logger.info(f"   â€¢ Total All Time: {self.total_trades_all_time} âœ…")

        # Guardar el estado sincronizado
        await self.save_intelligence()

        return True

    def get_statistics(self) -> Dict:
        """Retorna estadÃ­sticas completas del sistema autÃ³nomo"""
        rl_stats = self.rl_agent.get_statistics()
        opt_stats = self.parameter_optimizer.get_optimization_statistics()

        # Validar sincronizaciÃ³n
        sync_status = self.validate_sync()

        # Log warning si no estÃ¡n sincronizados
        if not sync_status['in_sync']:
            diffs = sync_status['differences']
            logger.warning(
                f"âš ï¸ DesincronizaciÃ³n detectada:\n"
                f"   RL vs Paper: {diffs['rl_vs_paper']} trades\n"
                f"   Processed vs Paper: {diffs['processed_vs_paper']} trades\n"
                f"   All-time vs Paper: {diffs['all_time_vs_paper']} trades\n"
                f"   Win Rate diff: {diffs['win_rate_diff']:.1f}%"
            )

        return {
            'active': self.active,
            'decision_mode': self.decision_mode,
            'total_trades_processed': self.total_trades_processed,
            'total_parameter_changes': self.total_parameter_changes,
            'rl_agent': rl_stats,
            'parameter_optimizer': opt_stats,
            'sync_status': sync_status,  # âœ… NUEVO: validaciÃ³n de sincronizaciÃ³n
            'last_optimization': self.last_optimization_time.isoformat(),
            'last_save': self.last_save_time.isoformat(),
            'current_parameters_count': len(self.current_parameters)
        }

    async def _notify_telegram(self, message: str):
        """EnvÃ­a notificaciÃ³n a Telegram"""
        if self.telegram_notifier:
            try:
                await self.telegram_notifier.send_status_message(message)
            except Exception as e:
                logger.warning(f"No se pudo enviar notificaciÃ³n: {e}")

    async def shutdown(self):
        """Apaga sistema autÃ³nomo y guarda estado final"""
        logger.info("ðŸ›‘ Apagando Sistema AutÃ³nomo...")

        self.active = False

        # Detener Git Auto-Backup
        await self.git_backup.stop_auto_backup()

        # Guardar inteligencia final
        await self.save_intelligence()

        await self._notify_telegram(
            "ðŸ›‘ **Sistema AutÃ³nomo Apagado**\n\n"
            f"ðŸ“Š Resumen de sesiÃ³n:\n"
            f"   â€¢ Trades procesados: {self.total_trades_processed}\n"
            f"   â€¢ ParÃ¡metros modificados: {self.total_parameter_changes} veces\n"
            f"   â€¢ Estados aprendidos: {len(self.rl_agent.q_table)}\n"
            f"   â€¢ Trials de optimizaciÃ³n: {self.parameter_optimizer.total_trials}\n\n"
            "âœ… Inteligencia guardada - lista para prÃ³ximo deploy"
        )

        logger.info("âœ… Sistema AutÃ³nomo apagado - Inteligencia preservada")

    async def manual_export(self) -> tuple[bool, str]:
        """
        Export manual de inteligencia (llamado por comando de Telegram)

        Returns:
            Tupla (success, export_file_path)
            - success: True si backup a Git fue exitoso
            - export_file_path: Path al archivo .json exportado
        """
        logger.info("ðŸ“¤ Export manual solicitado...")

        # Guardar inteligencia primero y obtener path del export
        export_path = await self.save_intelligence()

        # Realizar backup a Git
        success = await self.git_backup.perform_backup(manual=True)

        return success, export_path

    async def manual_import(self, file_path: str, merge: bool = False, force: bool = False) -> bool:
        """
        Import manual de inteligencia desde archivo (llamado por comando de Telegram)

        Args:
            file_path: Path al archivo .json con la inteligencia
            merge: Si True, combina con datos existentes. Si False, reemplaza
            force: Si True, ignora validaciÃ³n de checksum (para archivos editados)

        Returns:
            True si import fue exitoso
        """
        logger.info(f"ðŸ“¥ Import manual solicitado desde: {file_path} (merge={merge}, force={force})")

        # Importar el archivo (con force si se especifica)
        logger.debug(f"ðŸ“¥ Paso 1/3: Importando archivo con force={force}")
        success = self.persistence.import_from_file(file_path, force=force)

        if not success:
            logger.error("âŒ FallÃ³ la importaciÃ³n del archivo")
            return False

        logger.info("âœ… Paso 1/3 completado: Archivo importado exitosamente")

        # Recargar la inteligencia importada (con force para ignorar checksum)
        logger.debug(f"ðŸ“¥ Paso 2/3: Cargando estado con force={force}")
        loaded = self.persistence.load_full_state(force=force)

        if not loaded:
            logger.error("âŒ No se pudo cargar la inteligencia importada")
            return False

        logger.info("âœ… Paso 2/3 completado: Estado cargado exitosamente")

        # Restaurar todo el estado
        logger.debug("ðŸ“¥ Paso 3/3: Restaurando componentes del sistema")

        try:
            # Restaurar RL Agent (con o sin merge)
            logger.debug("  â€¢ Restaurando RL Agent...")
            if 'rl_agent' not in loaded:
                logger.error("âŒ No se encontrÃ³ 'rl_agent' en el archivo importado")
                return False

            self.rl_agent.load_from_dict(loaded['rl_agent'], merge=merge)
            logger.info("  âœ… RL Agent restaurado")

        except Exception as e:
            logger.error(f"âŒ Error restaurando RL Agent: {e}", exc_info=True)
            return False

        try:
            # Restaurar Parameter Optimizer (con o sin merge)
            logger.debug("  â€¢ Restaurando Parameter Optimizer...")
            if 'parameter_optimizer' not in loaded:
                logger.warning("âš ï¸ No se encontrÃ³ 'parameter_optimizer' en el archivo - usando estado vacÃ­o")
                # Continuar con optimizer vacÃ­o
            else:
                self.parameter_optimizer.load_from_dict(loaded['parameter_optimizer'], merge=merge)
            logger.info("  âœ… Parameter Optimizer restaurado")

        except Exception as e:
            logger.warning(f"âš ï¸ Error restaurando Parameter Optimizer (continuando): {e}")
            # No es crÃ­tico, continuar

        try:
            # Restaurar historial de cambios
            logger.debug("  â€¢ Restaurando historial de cambios...")
            if merge:
                # En merge, agregar cambios histÃ³ricos a los existentes
                imported_changes = loaded.get('change_history', [])
                self.change_history.extend(imported_changes)
                # Mantener solo Ãºltimos 100
                if len(self.change_history) > 100:
                    self.change_history = self.change_history[-100:]
                logger.info(f"  âœ… {len(imported_changes)} cambios histÃ³ricos agregados (total: {len(self.change_history)})")
            else:
                # En replace, reemplazar completamente
                self.change_history = loaded.get('change_history', [])
                logger.info(f"  âœ… HistÃ³rico de cambios restaurado: {len(self.change_history)} cambios")

        except Exception as e:
            logger.warning(f"âš ï¸ Error restaurando historial de cambios (continuando): {e}")
            # No es crÃ­tico, continuar

        try:
            # Restaurar metadata
            logger.debug("  â€¢ Restaurando metadata...")
            metadata = loaded.get('metadata', {})
            if metadata:
                if merge:
                    # En merge, solo actualizar si no existen
                    if not self.current_parameters:
                        self.current_parameters = metadata.get('current_parameters', {})
                    self.total_trades_processed += metadata.get('total_trades_processed', 0)

                    # total_trades_all_time SIEMPRE se acumula (nunca se resetea)
                    # Buscar en metadata primero, luego en rl_agent como fallback
                    imported_all_time = metadata.get('total_trades_all_time')
                    if imported_all_time is None:
                        # Fallback: usar total_experience_trades del RL agent si existe
                        rl_data = loaded.get('rl_agent', {})
                        imported_all_time = rl_data.get('total_experience_trades', metadata.get('total_trades_processed', 0))
                        logger.debug(f"  âš ï¸ total_trades_all_time no encontrado en metadata, usando RL agent: {imported_all_time}")

                    self.total_trades_all_time += imported_all_time
                    self.total_parameter_changes += metadata.get('total_parameter_changes', 0)
                    logger.info(f"  âœ… Metadata acumulada (trades totales: {self.total_trades_all_time}, max leverage: {self._calculate_max_leverage()}x)")
                else:
                    # En replace, reemplazar completamente
                    self.current_parameters = metadata.get('current_parameters', {})
                    self.total_trades_processed = metadata.get('total_trades_processed', 0)

                    # Cargar total_trades_all_time con mÃºltiples fallbacks
                    self.total_trades_all_time = metadata.get('total_trades_all_time')
                    if self.total_trades_all_time is None:
                        # Fallback 1: usar total_experience_trades del RL agent
                        rl_data = loaded.get('rl_agent', {})
                        self.total_trades_all_time = rl_data.get('total_experience_trades')
                        if self.total_trades_all_time is not None:
                            logger.debug(f"  âš ï¸ total_trades_all_time no en metadata, usando RL agent total_experience_trades: {self.total_trades_all_time}")
                        else:
                            # Fallback 2: usar total_trades_processed
                            self.total_trades_all_time = metadata.get('total_trades_processed', 0)
                            logger.debug(f"  âš ï¸ total_trades_all_time no encontrado, usando total_trades_processed: {self.total_trades_all_time}")

                    # Si aÃºn es None o 0, usar el total_trades del RL agent que ya fue cargado
                    if self.total_trades_all_time == 0 and hasattr(self, 'rl_agent'):
                        self.total_trades_all_time = self.rl_agent.total_trades
                        logger.debug(f"  âš ï¸ Usando total_trades del RL agent cargado: {self.total_trades_all_time}")

                    self.total_parameter_changes = metadata.get('total_parameter_changes', 0)
                    self.decision_mode = metadata.get('decision_mode', 'BALANCED')
                    logger.info(f"  âœ… Metadata restaurada (trades totales: {self.total_trades_all_time}, max leverage: {self._calculate_max_leverage()}x)")
            else:
                logger.warning("âš ï¸ No se encontrÃ³ metadata en el archivo - usando valores por defecto")
                # Usar total_trades del RL agent como fallback si no hay metadata
                if hasattr(self, 'rl_agent'):
                    self.total_trades_all_time = self.rl_agent.total_trades
                    logger.debug(f"  âš ï¸ Sin metadata, usando total_trades del RL agent: {self.total_trades_all_time}")

        except Exception as e:
            logger.warning(f"âš ï¸ Error restaurando metadata (continuando): {e}")
            # No es crÃ­tico, continuar
            # Intentar recuperar de RL agent como Ãºltimo recurso
            if hasattr(self, 'rl_agent'):
                self.total_trades_all_time = self.rl_agent.total_trades
                logger.debug(f"  âš ï¸ ExcepciÃ³n en metadata, usando RL agent total_trades: {self.total_trades_all_time}")

        try:
            # Restaurar performance history
            logger.debug("  â€¢ Restaurando performance history...")
            perf_history = loaded.get('performance_history', {})
            if perf_history.get('recent_performance'):
                if merge:
                    # En merge, agregar a la historia existente
                    self.performance_history.extend(perf_history['recent_performance'])
                    # Mantener solo Ãºltimos 100
                    if len(self.performance_history) > 100:
                        self.performance_history = self.performance_history[-100:]
                else:
                    # En replace, reemplazar
                    self.performance_history = perf_history['recent_performance']
                logger.info(f"  âœ… Performance history restaurada ({len(self.performance_history)} entradas)")
            else:
                logger.warning("âš ï¸ No se encontrÃ³ performance history en el archivo")

        except Exception as e:
            logger.warning(f"âš ï¸ Error restaurando performance history (continuando): {e}")
            # No es crÃ­tico, continuar

        # Ã‰xito completo
        mode_str = "combinada" if merge else "restaurada"
        force_str = " (FORCE MODE)" if force else ""
        logger.info(f"âœ… Paso 3/3 completado: Componentes restaurados")
        logger.info(f"ðŸŽ‰ Inteligencia importada y {mode_str} completamente{force_str}")

        # Resumen final
        logger.info(
            f"ðŸ“Š Resumen de importaciÃ³n:\n"
            f"  â€¢ RL Agent: {self.rl_agent.total_trades} trades, {len(self.rl_agent.q_table)} estados\n"
            f"  â€¢ Total trades all time: {self.total_trades_all_time}\n"
            f"  â€¢ Max leverage: {self._calculate_max_leverage()}x\n"
            f"  â€¢ Parameter changes: {self.total_parameter_changes}\n"
            f"  â€¢ Change history: {len(self.change_history)} cambios"
        )

        # ===== RESTAURAR PAPER TRADING (USANDO MÃ‰TODO CORRECTO) =====
        try:
            logger.info("ðŸ“¥ Verificando Paper Trading en archivo...")

            if 'paper_trading' in loaded and loaded['paper_trading']:
                paper_data = loaded['paper_trading']

                # Verificar estructura del export (NUEVO FORMATO con counters)
                if 'counters' in paper_data and 'closed_trades' in paper_data:
                    total_trades = paper_data.get('counters', {}).get('total_trades', 0)
                    closed_trades_count = len(paper_data.get('closed_trades', []))

                    if total_trades > 0:
                        logger.info(f"ðŸ“Š Paper trading detectado en export:")
                        logger.info(f"   â€¢ Total trades: {total_trades}")
                        logger.info(f"   â€¢ Closed trades: {closed_trades_count}")
                        logger.info(f"   â€¢ Balance: ${paper_data.get('balance', 0):,.2f}")

                        # Verificar que paper_trader existe
                        if not hasattr(self, 'paper_trader') or self.paper_trader is None:
                            logger.error("âŒ CRÃTICO: self.paper_trader NO EXISTE")
                            logger.error("   El paper_trader debe asignarse desde main.py ANTES de import")
                            logger.error("   Continuando sin restaurar paper trading...")
                        elif not hasattr(self.paper_trader, 'portfolio'):
                            logger.error("âŒ CRÃTICO: paper_trader.portfolio NO EXISTE")
                        else:
                            # TODO LISTO - Usar mÃ©todo restore_from_state()
                            logger.info("âœ… paper_trader existe - ejecutando restore_from_state()...")

                            try:
                                success = self.paper_trader.portfolio.restore_from_state(paper_data)

                                if success:
                                    # Verificar que realmente se restaurÃ³
                                    actual_trades = self.paper_trader.portfolio.total_trades
                                    actual_closed = len(self.paper_trader.portfolio.closed_trades)
                                    actual_balance = self.paper_trader.portfolio.balance

                                    logger.info(f"âœ… Paper Trading restaurado exitosamente:")
                                    logger.info(f"   â€¢ Total trades: {actual_trades}")
                                    logger.info(f"   â€¢ Closed trades: {actual_closed}")
                                    logger.info(f"   â€¢ Balance: ${actual_balance:,.2f}")

                                    # VerificaciÃ³n de integridad
                                    if actual_trades != total_trades:
                                        logger.warning(f"âš ï¸ Trades: esperado={total_trades}, actual={actual_trades}")
                                else:
                                    logger.error("âŒ restore_from_state() retornÃ³ False")
                                    logger.error("   Revisa logs de Portfolio para mÃ¡s detalles")

                            except Exception as e:
                                logger.error(f"âŒ EXCEPCIÃ“N al ejecutar restore_from_state(): {e}", exc_info=True)
                    else:
                        logger.warning("âš ï¸ Paper trading en export pero con 0 trades")
                else:
                    logger.warning("âš ï¸ Paper trading en export pero formato incorrecto (sin counters/closed_trades)")
                    logger.warning("   Puede ser un export antiguo - usa un export reciente creado con /export")
            else:
                logger.info("â„¹ï¸ No hay paper trading en el export")

        except Exception as e:
            logger.error(f"âŒ Error restaurando Paper Trading: {e}", exc_info=True)
            # No es crÃ­tico, continuar
        # ===== FIN RESTAURAR PAPER TRADING =====

        # ===== RESTAURAR ML TRAINING BUFFER =====
        # El training_buffer contiene las features necesarias para entrenar el ML
        # Sin estas features, el ML no puede reentrenarse con los trades importados
        try:
            # Intentar cargar ml_training_data primero (formato nuevo)
            if 'ml_training_data' in loaded and loaded['ml_training_data']:
                logger.info("ðŸ§  Restaurando ML Training Data...")

                # Verificar que tengamos acceso al ml_system
                if hasattr(self, 'market_monitor') and self.market_monitor:
                    if hasattr(self.market_monitor, 'ml_system') and self.market_monitor.ml_system:
                        ml_system = self.market_monitor.ml_system
                        training_data = loaded['ml_training_data']

                        # Asignar ml_training_data al ml_system
                        ml_system.ml_training_data = training_data
                        logger.info(f"  âœ… ML training data restaurado: {len(training_data)} muestras")

                        # TambiÃ©n copiar a training_buffer para compatibilidad
                        if not hasattr(ml_system, 'training_buffer') or not ml_system.training_buffer:
                            ml_system.training_buffer = training_data
                            logger.info(f"  âœ… Training buffer sincronizado desde ml_training_data")

            # Fallback: cargar ml_training_buffer (formato antiguo)
            elif 'ml_training_buffer' in loaded and loaded['ml_training_buffer']:
                logger.info("ðŸ§  Restaurando ML Training Buffer...")

                # Verificar que tengamos acceso al ml_system
                if hasattr(self, 'market_monitor') and self.market_monitor:
                    if hasattr(self.market_monitor, 'ml_system') and self.market_monitor.ml_system:
                        ml_system = self.market_monitor.ml_system
                        buffer_data = loaded['ml_training_buffer']

                        # Restaurar training_buffer
                        ml_system.training_buffer = buffer_data
                        # TambiÃ©n asignar a ml_training_data para compatibilidad
                        ml_system.ml_training_data = buffer_data
                        logger.info(f"  âœ… Training buffer restaurado: {len(buffer_data)} features")
                        logger.info(f"  âœ… ML training data sincronizado desde buffer")

                        # NUEVO: Crear mapa trade_id â†’ features para fallback
                        # Esto permite que _get_features_for_trades() encuentre features
                        # incluso si el training_buffer se reorganiza
                        imported_features_map = {}
                        for record in buffer_data:
                            if 'trade_id' in record and 'features' in record:
                                imported_features_map[record['trade_id']] = record['features']

                        ml_system.imported_features = imported_features_map
                        logger.info(f"  âœ… Features indexadas: {len(imported_features_map)} trade IDs")

                        # Guardar a disco
                        ml_system._save_buffer()
                        logger.debug(f"  ðŸ’¾ Training buffer guardado en disco")
                    else:
                        logger.warning("âš ï¸ ML System no disponible, no se puede restaurar training_buffer")
                else:
                    logger.warning("âš ï¸ Market Monitor no disponible, no se puede restaurar training_buffer")
            else:
                logger.debug("â„¹ï¸  No se encontrÃ³ 'ml_training_data' ni 'ml_training_buffer' en el archivo importado")

        except Exception as e:
            logger.error(f"âŒ Error restaurando ML Training Data/Buffer: {e}", exc_info=True)
            # No es crÃ­tico, continuar
        # ===== FIN RESTAURAR ML TRAINING DATA/BUFFER =====

        # ===== PARCHE DIRECTO PARA total_trades_all_time =====
        # Verificar si total_trades_all_time quedÃ³ en 0
        if self.total_trades_all_time == 0:
            logger.warning("âš ï¸ CRÃTICO: total_trades_all_time estÃ¡ en 0, aplicando parche...")

            # Intento 1: Desde RL Agent ya cargado
            if hasattr(self.rl_agent, 'total_trades') and self.rl_agent.total_trades > 0:
                self.total_trades_all_time = self.rl_agent.total_trades
                logger.warning(f"âš ï¸ PARCHE APLICADO: total_trades_all_time = {self.total_trades_all_time} (desde RL Agent)")

            # Intento 2: Desde el archivo cargado
            elif 'rl_agent' in loaded:
                rl_data = loaded['rl_agent']
                experience_trades = rl_data.get('total_experience_trades', 0)
                if experience_trades > 0:
                    self.total_trades_all_time = experience_trades
                    logger.warning(f"âš ï¸ PARCHE APLICADO: total_trades_all_time = {experience_trades} (desde total_experience_trades)")
                else:
                    # Intento 3: Desde statistics del RL agent
                    stats = rl_data.get('statistics', {})
                    total_from_stats = stats.get('total_trades', 0)
                    if total_from_stats > 0:
                        self.total_trades_all_time = total_from_stats
                        logger.warning(f"âš ï¸ PARCHE APLICADO: total_trades_all_time = {total_from_stats} (desde statistics)")

        # Log final para confirmar
        logger.info(f"ðŸŽ¯ VERIFICACIÃ“N FINAL: total_trades_all_time = {self.total_trades_all_time}")
        logger.info(f"ðŸŽ¯ Max leverage desbloqueado = {self._calculate_max_leverage()}x")
        # ===== FIN DEL PARCHE =====

        # ===== PARCHE DIRECTO PARA total_parameter_changes =====
        # Verificar tambiÃ©n total_parameter_changes
        if self.total_parameter_changes == 0:
            logger.warning("âš ï¸ CRÃTICO: total_parameter_changes estÃ¡ en 0, aplicando parche...")

            # Buscar en metadata
            metadata = loaded.get('metadata', {})
            param_changes = metadata.get('total_parameter_changes', 0)
            if param_changes > 0:
                self.total_parameter_changes = param_changes
                logger.warning(f"âš ï¸ PARCHE APLICADO: total_parameter_changes = {param_changes}")
            else:
                # Buscar en parameter_optimizer
                param_opt = loaded.get('parameter_optimizer', {})
                total_opts = param_opt.get('total_optimizations', 0)
                if total_opts > 0:
                    self.total_parameter_changes = total_opts
                    logger.warning(f"âš ï¸ PARCHE APLICADO: total_parameter_changes = {total_opts} (desde total_optimizations)")

        # Log final de verificaciÃ³n
        logger.info(f"ðŸŽ¯ VERIFICACIÃ“N FINAL: total_parameter_changes = {self.total_parameter_changes}")
        # ===== FIN DEL PARCHE =====

        # ===== AUTO-ENTRENAMIENTO ML SI HAY DATOS =====
        # Si tenemos suficientes trades y features, entrenar ML automÃ¡ticamente
        try:
            if hasattr(self, 'market_monitor') and self.market_monitor:
                if hasattr(self.market_monitor, 'ml_system') and self.market_monitor.ml_system:
                    ml_system = self.market_monitor.ml_system

                    # Verificar si tenemos trades y features
                    if hasattr(self, 'paper_trader') and self.paper_trader:
                        paper_trader = self.paper_trader
                        stats = paper_trader.portfolio.get_statistics()
                        total_trades = stats.get('total_trades', 0)
                        buffer_size = len(ml_system.training_buffer)
                        imported_features_size = len(ml_system.imported_features)

                        logger.info(f"ðŸ¤– Verificando posibilidad de auto-entrenamiento ML:")
                        logger.info(f"   â€¢ Trades: {total_trades}")
                        logger.info(f"   â€¢ Training buffer: {buffer_size} registros")
                        logger.info(f"   â€¢ Features importadas: {imported_features_size} trades")

                        # Entrenar si tenemos 40+ trades y al menos 25 features
                        if total_trades >= 40 and (buffer_size >= 25 or imported_features_size >= 25):
                            logger.info("ðŸš€ Iniciando auto-entrenamiento ML con datos importados...")

                            # Forzar entrenamiento con threshold reducido
                            ml_system.force_retrain(
                                min_samples_override=25,
                                external_paper_trader=paper_trader
                            )

                            # Verificar resultado
                            model_info = ml_system.trainer.get_model_info()
                            if model_info.get('available'):
                                metrics = model_info.get('metrics', {})
                                logger.info("âœ… ML ENTRENADO EXITOSAMENTE con datos importados:")
                                logger.info(f"   â€¢ Accuracy: {metrics.get('test_accuracy', 0):.1%}")
                                logger.info(f"   â€¢ Precision: {metrics.get('test_precision', 0):.1%}")
                                logger.info(f"   â€¢ F1 Score: {metrics.get('test_f1', 0):.3f}")
                            else:
                                logger.warning("âš ï¸ Auto-entrenamiento completado pero modelo no disponible")
                        else:
                            logger.info("â„¹ï¸  No hay suficientes datos para auto-entrenamiento ML")
                            logger.info("   El ML entrenarÃ¡ automÃ¡ticamente cuando haya 40+ trades")

        except Exception as e:
            logger.error(f"âŒ Error en auto-entrenamiento ML: {e}", exc_info=True)
            logger.info("   El ML entrenarÃ¡ mÃ¡s tarde cuando haya datos suficientes")
        # ===== FIN AUTO-ENTRENAMIENTO ML =====

        return True

    def update_from_trade_result(self, closed_info: Dict, reward: float):
        """
        Actualiza RL Agent con resultado de trade cerrado (v2.0 Binance)
        MÃ©todo simplificado que toma closed_info de PositionMonitor

        Args:
            closed_info: InformaciÃ³n del trade cerrado desde Binance
                - symbol: Par (ej: BTCUSDT)
                - side: LONG o SHORT
                - realized_pnl: P&L en USDT
                - realized_pnl_pct: P&L en porcentaje
                - leverage: Apalancamiento usado
                - entry_price: Precio de entrada
                - exit_price: Precio de salida
            reward: Reward calculado (usualmente el realized_pnl)
        """
        try:
            # Extraer datos del trade
            symbol = closed_info.get('symbol', 'UNKNOWN')
            side = closed_info.get('side', 'LONG')
            realized_pnl = closed_info.get('realized_pnl', 0)
            realized_pnl_pct = closed_info.get('realized_pnl_pct', 0)
            leverage = closed_info.get('leverage', 1)

            # DEDUPLICACIÃ“N: Evitar procesar el mismo trade dos veces
            # (test_mode y position_monitor pueden notificar el mismo cierre)
            current_time = datetime.now().timestamp()

            # Limpiar trades antiguos (mÃ¡s de 10 segundos)
            symbols_to_remove = []
            for sym, (ts, _) in self._recently_processed_trades.items():
                if current_time - ts > 10:
                    symbols_to_remove.append(sym)
            for sym in symbols_to_remove:
                del self._recently_processed_trades[sym]

            # Verificar si ya procesamos este trade recientemente
            if symbol in self._recently_processed_trades:
                prev_ts, prev_pnl = self._recently_processed_trades[symbol]
                time_diff = current_time - prev_ts

                # Si el P&L es similar (dentro de 5%) y fue hace menos de 10 segundos, es duplicado
                if time_diff < 10 and abs(prev_pnl - realized_pnl) < abs(prev_pnl * 0.05):
                    logger.warning(
                        f"âš ï¸ Trade duplicado detectado y IGNORADO: {symbol} | "
                        f"P&L: ${realized_pnl:+.2f} | "
                        f"Tiempo desde Ãºltimo: {time_diff:.1f}s | "
                        f"RazÃ³n: {closed_info.get('reason', 'N/A')}"
                    )
                    return

            # Registrar este trade como procesado
            self._recently_processed_trades[symbol] = (current_time, realized_pnl)

            # Log de cuÃ¡l fuente estÃ¡ notificando (trade_id puede ser int o string)
            trade_id = str(closed_info.get('trade_id', ''))
            source = "Test Mode" if "test_" in trade_id else "Position Monitor"
            logger.info(f"ðŸ“¥ Trade notification from: {source}")

            # CRÃTICO: Ignorar Position Monitor cuando Test Mode estÃ¡ activo
            # (Test Mode ya notificÃ³ con el P&L correcto, Position Monitor tendrÃ­a P&L diferente)
            if source == "Position Monitor" and self.test_mode_active:
                logger.info(f"â­ï¸ Ignorando notificaciÃ³n de Position Monitor (Test Mode activo)")
                return  # No procesar esta notificaciÃ³n duplicada

            # Incrementar contador global
            self.total_trades_all_time += 1

            # Crear state simplificado para RL Agent
            # (el state completo se crearÃ¡ internamente en learn_from_trade)
            state_dict = {
                'symbol': symbol,
                'side': side,
                'leverage': leverage,
                'rsi': 50,  # Valores default (el RL usa el next_state principalmente)
                'regime': 'SIDEWAYS',
                'orderbook': 'NEUTRAL',
                'volatility': 'medium'
            }

            state = self.rl_agent.get_state_representation(state_dict)

            # Normalizar reward (P&L en porcentaje es mÃ¡s Ãºtil que absoluto)
            normalized_reward = realized_pnl_pct / 100.0  # -10% â†’ -0.1, +5% â†’ 0.05

            # RL Agent aprende del trade
            done = abs(realized_pnl_pct) > 15  # Episodio termina en grandes wins/losses
            self.rl_agent.learn_from_trade(
                reward=normalized_reward,
                next_state=state,
                done=done
            )

            # Log de aprendizaje
            emoji = "âœ…" if realized_pnl > 0 else "âŒ"
            logger.info(
                f"{emoji} RL LEARNING: {symbol} {side} | "
                f"P&L: {realized_pnl_pct:+.2f}% | "
                f"Leverage: {leverage}x | "
                f"Reward: {normalized_reward:+.3f} | "
                f"Total trades: {self.total_trades_all_time}"
            )

            # Experience Replay periÃ³dico
            if self.rl_agent.total_trades % 10 == 0:
                self.rl_agent.replay_experience(batch_size=32)
                logger.debug(f"ðŸ”„ Experience replay ejecutado ({self.rl_agent.total_trades} trades)")

        except Exception as e:
            logger.error(f"âŒ Error in update_from_trade_result: {e}", exc_info=True)

    def export_intelligence(self) -> Dict:
        """Exporta toda la inteligencia aprendida"""
        try:
            intelligence_data = {
                "version": "2.0",
                "timestamp": datetime.now().isoformat(),
                "rl_agent": self.rl_agent.save_to_dict(),
                "parameter_optimizer": self.parameter_optimizer.save_to_dict(),
                "performance_history": {
                    'total_trades': len(self.performance_history),
                    'recent_performance': self.performance_history[-100:] if self.performance_history else []
                },
                "metadata": {
                    'current_parameters': self.current_parameters,
                    'total_trades_processed': self.total_trades_processed,
                    'total_trades_all_time': self.total_trades_all_time,
                    'max_leverage_unlocked': self._calculate_max_leverage(),
                    'total_parameter_changes': self.total_parameter_changes,
                    'last_optimization': self.last_optimization_time.isoformat(),
                    'decision_mode': self.decision_mode
                },

                # âœ¨ AÃ‘ADIR LEARNING SYSTEM
                "trade_management_learning": self.get_trade_management_learning_data(),
            }

            # AÃ±adir paper trading si existe
            if hasattr(self, 'paper_trader') and self.paper_trader:
                intelligence_data['paper_trading'] = self.paper_trader.portfolio.get_full_state_for_export()
                logger.debug(
                    f"ðŸ“¤ Exportando paper trading: "
                    f"{len(intelligence_data['paper_trading'].get('closed_trades', []))} trades"
                )

            # AÃ±adir ML training buffer si existe
            ml_training_buffer = []
            if hasattr(self, 'market_monitor') and self.market_monitor:
                if hasattr(self.market_monitor, 'ml_system') and self.market_monitor.ml_system:
                    ml_system = self.market_monitor.ml_system
                    if hasattr(ml_system, 'training_buffer'):
                        ml_training_buffer = ml_system.training_buffer
                        intelligence_data['ml_training_buffer'] = ml_training_buffer
                        logger.debug(f"ðŸ§  ML Training Buffer incluido en export: {len(ml_training_buffer)} features")

            return intelligence_data

        except Exception as e:
            logger.error(f"Error exporting intelligence: {e}", exc_info=True)
            return {}

    def get_trade_management_learning_data(self) -> Dict:
        """Obtiene datos del learning system del Trade Manager"""
        try:
            # OpciÃ³n 1: Si trade_manager estÃ¡ en autonomy_controller
            if hasattr(self, 'trade_manager') and self.trade_manager:
                return self.trade_manager.learning.export_to_json()

            # OpciÃ³n 2: Cargar desde archivo si no hay referencia
            from pathlib import Path
            import json
            filepath = 'data/trade_management_learning.json'
            if Path(filepath).exists():
                with open(filepath, 'r') as f:
                    return json.load(f)

            return {}

        except Exception as e:
            logger.error(f"Error getting trade management learning: {e}", exc_info=True)
            return {}

